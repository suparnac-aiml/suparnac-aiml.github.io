{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2396a20-4117-4926-985a-3e3014c41448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image, ImageEnhance\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d923cc31-6902-48c5-a57b-2417b0cfc414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Mac GPU support (MPS)\n",
    "device = torch.device(\"mps\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f26913c-8759-41f0-b6c7-79f6e0c04a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suparnac/dev_envs/cv_env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/suparnac/dev_envs/cv_env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained ResNet18 architecture and adjust final layer (we did this in ASL_model_training)\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 29)  # 29 ASL classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54fb7b0b-4ea1-4702-9bac-927c90f79eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=29, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load saved best model weights (we the best_model_asl.pth that was saved from model training script)\n",
    "model.load_state_dict(torch.load('/Users/suparnac/dev_envs/CV_Projects/Amarican_Sign_Language/best_model_asl.pth', map_location=device))\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "465f24b2-5990-40f9-ab2b-deafeca3f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class-to-index mapping (exactly as used during training)\n",
    "class_to_idx = {\n",
    "    'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6,\n",
    "    'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13,\n",
    "    'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20,\n",
    "    'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25, 'del': 26, 'nothing': 27, 'space': 28\n",
    "}\n",
    "\n",
    "# Reverse mapping from idx to class label\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca7f507c-6ceb-47ed-acde-8c2b15873d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms matching training preprocessing exactly \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebc1a33f-b040-46cf-b53c-5bd27f7244e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize webcam capture with AVFoundation backend for Mac\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_AVFOUNDATION)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "476ec7bb-2265-4e1b-8074-76e2a0df2354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ROI (Region of Interest) for hand detection\n",
    "roi_top, roi_bottom = 100, 400\n",
    "roi_left, roi_right = 200, 500\n",
    "\n",
    "# Prediction smoothing using a sliding window\n",
    "prediction_window = deque(maxlen=5)  # Store last 5 predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08c40656-d7d9-48e4-b740-266f2252b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \"\"\"Enhanced preprocessing for better real-time performance\"\"\"\n",
    "    # Convert to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Enhance contrast and brightness\n",
    "    pil_image = Image.fromarray(rgb_frame)\n",
    "    \n",
    "    # Enhance contrast slightly\n",
    "    enhancer = ImageEnhance.Contrast(pil_image)\n",
    "    pil_image = enhancer.enhance(1.2)\n",
    "    \n",
    "    # Enhance brightness slightly\n",
    "    enhancer = ImageEnhance.Brightness(pil_image)\n",
    "    pil_image = enhancer.enhance(1.1)\n",
    "    \n",
    "    return pil_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "993484d1-d872-49d8-b0af-0ffb3fe1e0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ASL detection with improved performance...\n",
      "Position your hand in the green ROI box for best results.\n",
      "Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "def get_smoothed_prediction(predictions_queue):\n",
    "    \"\"\"Get most frequent prediction from the queue\"\"\"\n",
    "    if not predictions_queue:\n",
    "        return \"nothing\"\n",
    "    \n",
    "    # Count occurrences of each prediction\n",
    "    prediction_counts = {}\n",
    "    for pred in predictions_queue:\n",
    "        prediction_counts[pred] = prediction_counts.get(pred, 0) + 1\n",
    "    \n",
    "    # Return most frequent prediction\n",
    "    return max(prediction_counts, key=prediction_counts.get)\n",
    "\n",
    "print(\"Starting ASL detection with improved performance...\")\n",
    "print(\"Position your hand in the green ROI box for best results.\")\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ffc37af-7475-4668-986c-2efad22722fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "    \n",
    "    # Flip frame horizontally for mirror effect (more natural for user)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Draw ROI rectangle on frame (for hand placement/ place your hand in that region or better accuracy)\n",
    "    cv2.rectangle(frame, (roi_left, roi_top), (roi_right, roi_bottom), (0, 255, 0), 2)\n",
    "    cv2.putText(frame, \"Position hand here\", (roi_left, roi_top-10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)\n",
    "    \n",
    "    # Extract ROI from frame\n",
    "    roi = frame[roi_top:roi_bottom, roi_left:roi_right]\n",
    "    \n",
    "    # Check if ROI is valid\n",
    "    if roi.size > 0:\n",
    "        try:\n",
    "            processed_image = preprocess_frame(roi)\n",
    "  \n",
    "            input_tensor = transform(processed_image).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_tensor)\n",
    "                probabilities = torch.softmax(outputs, dim=1)\n",
    "                confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "                \n",
    "                predicted_class = idx_to_class[int(predicted_idx)]\n",
    "                confidence_score = float(confidence)\n",
    "\n",
    "                if confidence_score > 0.5:  # Confidence threshold\n",
    "                    prediction_window.append(predicted_class)\n",
    "                else:\n",
    "                    prediction_window.append(\"nothing\")  # Low confidence\n",
    "\n",
    "            smoothed_prediction = get_smoothed_prediction(prediction_window)\n",
    "\n",
    "            display_text = f'Prediction: {smoothed_prediction.upper()}'\n",
    "            confidence_text = f'Confidence: {confidence_score:.2f}'\n",
    "\n",
    "            color = (0, 0, 255)  # Red color\n",
    "            \n",
    "            cv2.putText(frame, display_text, (10, 40), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3)\n",
    "            cv2.putText(frame, confidence_text, (10, 80), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during inference: {e}\")\n",
    "            cv2.putText(frame, \"Processing Error\", (10, 40), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    \n",
    "    # Show frame\n",
    "    cv2.imshow('Improved ASL Real-time Detection', frame)\n",
    "    \n",
    "    # Exit if 'q' pressed\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif key == ord('r'):  # Reset prediction window\n",
    "        prediction_window.clear()\n",
    "        print(\"Prediction window reset\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01115642-ef3b-47db-9fef-49c4bb5dfe1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cv_env)",
   "language": "python",
   "name": "cv_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
